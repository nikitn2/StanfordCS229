\documentclass[11pt]{article}
\usepackage{spikey}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{soul}
\usepackage{float}
\usepackage{graphicx}
\usepackage{subfig}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{chngcntr}
\usepackage{centernot}
\usepackage{datetime}
\usepackage[shortlabels]{enumitem}

\usepackage[margin=1truein]{geometry}
\usepackage{setspace}
\linespread{1.15}

\counterwithin{equation}{section}
\newcommand{\upi}[0]{^{(i)}}
\newcommand{\upj}[0]{^{(j)}}
\newcommand{\upk}[0]{^{(k)}}

\title{CS229 Problem Set 2}
\date{\today}
\author{Tianyu Du}
\begin{document}
	\maketitle
	\newpage
	\section{Problem 1 Logistic Regression: Training Stability}
	\subsection{1(a)}
	\paragraph{Comment} The parameter trained on dataset A converges after around 30,000 iterations of gradient ascent. In contrast, on dataset B, the training algorithm does not reach convergence within a reasonable amount of time.
	\newpage
	\subsection{1(b)}
	\begin{figure}[h]
	    \centering
	    \subfloat[Dataset A]{{\includegraphics[width=0.5\linewidth]{./src/stability/ds1_a.png}}}
	    \subfloat[Dataset B]{{\includegraphics[width=0.5\linewidth]{./src/stability/ds1_b_abline.png}}}
	    \caption{Two Datasets}
	\end{figure}
	\paragraph{Comment} From the plot of two datasets we can see that dataset B is more \emph{linearly separable} than dataset A (See the oblique line in the plot). There exists intermixing of data points belonging to both classes in dataset A, but fewer can be observed in dataset B. The linear separability of dataset B causes the instability of logistic regression on it. In the scenario of dataset B, we claim that the gradient descent algorithm increases $\theta$ indefinitely without convergence.
	\begin{proof}
		Let $\mc{A}$ and $\mc{B}$ denote two datasets respectively.
%		The gradient descent for logistic regression works as
%		\begin{align}
%			\theta \leftarrow \theta + \alpha \sum_{i=1}^n \left(y\upi - h_\theta(x\upi)\right) x\upi
%		\end{align}
		And the log-likelihood looks like
		\begin{align}
			\ell (\theta) &= \sum_{i=1}^{n} y^{(i)} \log h_\theta\left(x^{(i)}\right)+\left(1-y^{(i)}\right) \log \left(1-h_\theta\left(x^{(i)}\right)\right) \\
			&= \underbrace{\sum_{i: y\upi = 0} \log \left(1 - h_\theta(x\upi)\right)}_{P} + 
			\underbrace{\sum_{i: y\upi = 1} \log h_\theta(x\upi)}_{Q}
		\end{align}
		Suppose the threshold for classification is 0.5, and dataset $\mc{B}$ can be separated perfectly with some $\theta \in \R^d$. That's, for (almost) every sample $i \in \mc{B}$ such that $y\upi = 0$, $\theta^T x\upi < 0$ and for (almost) every $y\upi = 1$, $\theta^T x\upi > 0$. As a result, for every $\theta^t$ that separate dataset $\mc{B}$ almost perfectly, suppose $\theta^t$ is inflated to $\theta^{t+1} := (1 + \varepsilon) \theta^t$ for some $\varepsilon > 0$, for those negative samples ($y\upi = 0$), $\theta^T x\upi$ changes to $(1 + \varepsilon) \theta^T x\upi < \theta^T x\upi < 0$, and $h_\theta(x\upi)$ decreases. As a result, $P$ increases. Similarly, for those positive samples, $\theta^T x\upi$ changes to $(1 + \varepsilon) \theta^T x\upi > \theta^T x\upi > 0$, and $h_\theta(x\upi)$, so does $Q$.\\
		It is shown that for each $\theta^t$ separates dataset perfectly (or at least almost perfectly, because if the misclassified group is small, we can safely ignore the contribution to likelihood function from samples from the misclassified group), the log likelihood is increased when $\theta$ is inflated, and the resulted $\theta^{t+1}$ is still a (almost) perfect-separating parameter. Also, there is no upper bound on $\norm{\theta}$, therefore, the gradient ascend algorithm will run indefinitely to increase $\ell(\theta)$ by inflation the norm of $\theta$.\\
		For dataset like $\mc{A}$, for each $\theta$, the misclassified group provides significant contributions to $\ell(\theta)$, inflating $\theta$ comes with the cost of reduced likelihood on samples from the misclassified groups. By the nature of $\log$ function, the cost of likelihood reduction on misclassified group will eventually overcome the the likelihood gain by expanding $\theta$, and $\ell(\theta)$ cannot be risen to infinity on this kind of datasets. Therefore, the algorithm stops at some $\theta^*$.
	\end{proof}
	\newpage
	\subsection{1(c)}
		\paragraph{Comment}
		\begin{enumerate}[(i)]
			\item (NO) A different constant learning rate won't help because the $\nabla_\theta \ell(\theta)$ does not vanish, and the update step $\alpha \nabla_\theta \ell(\theta)$ is always significant. Therefore, convergence is still not achievable.
			\item (NO) Annealing the learning rate will not mitigate the problem because this does not change the fact that the maximum of $\ell$ is achieved when $\theta \to \infty$, it really depends on the gradient of $\nabla_\theta J(\theta)$, if $\norm{\nabla_\theta J(\theta)}$ does not increase more than quadratic order, then multiplying $\frac{1}{t^2}$ could help the algorithm to converge, but still, this kind of convergence just indicates the algorithm halts, but does not indicate the $\theta$ converges to a minimum. If the norm of gradient increases faster than quadratic order, the algorithm will not converge neither. Therefore, learning rate decaying may help the training seems to converge, but it does not cure the underlying problem ($\theta_{MLE}$ does not exist).
			\item (NO) Linear scaling does not change the fact that dataset is well (linearly) separable, $\theta$ is still going to explode indefinitely.
			\item (YES) Adding regularization helps. As mentioned before, the main cause of interminability of gradient ascent is $\ell(\theta)$ can always be increased by inflating $\theta$ (i.e. increasing $\norm{\theta}$). The regularizing term prevents $\theta$ from exploding and enforce the convergence.
			\item (YES) Give the variance of noise is large enough, the noise helps eliminate the linear separability of data, so that the dataset becomes intermixing.
		\end{enumerate}
	
	\newpage
	\subsection{1(d)}
	\paragraph{Comment} SVM is more robust against linearly separable dataset. SVM is seeking to construct a hyperplane $w^T x + b$ to maximize the geometric margin $\gamma$, while controlling $\norm{w} = 1$. Note that changing $b$ would shift the position of the hyperplane, therefore at optimal, $b^* < \infty$. Also, the performance measure of SVM using geometric margin is invariant to scales of $(w, b)$. Therefore, given the constraint on $\norm{w}$, SVM using geometric margin does not suffer from the parameter exploding problem (optimal is achieved when $\theta \to \infty$), and the optimal $(w^*, b^*)$ will both be finite.
	
	\newpage
	\section{Problem 2 Spam Classification}
	\subsection{2(a)}
	\paragraph{Result} \texttt{Size of dictionary: 1722}
	
	\newpage
	\subsection{2(b)}
	\paragraph{Result} \texttt{Naive Bayes had an accuracy of 0.978494623655914 on the testing set}
	
	\newpage
	\subsection{2(c)}
	\paragraph{Result} \texttt{The top 5 indicative words for Naive Bayes are:  ['claim', 'won', 'prize', 'tone', 'urgent!']}
	
	\newpage
	\subsection{2(d)}
	\paragraph{Result}
	\begin{verbatim}
		The optimal SVM radius was 0.1
		The SVM model had an accuracy of 0.9695340501792115 on the testing set
	\end{verbatim}
	
	\newpage
	\section{Problem 3 Constructing Kernels}
	\begin{notation}
		In the question, we use $K(\cdot)$ to denote the kernel mapping, and use $K$ to denote the corresponding kernel matrix generated by some collection $(x\upi)_{i=1}^n \subset \R^d$. Specifically, $K_{i, j} := K(x\upi, x^{(j)})$.
	\end{notation}
	\subsection{3(a)}
	\begin{proof}
		$K(\cdot)$ is a kernel. Let $(x\upi)_{i=1}^n \subset \R^d$ be any sequence of $n$ vectors from $\R^d$, let $K_1$ and $K_2$ denote the corresponding kernel metrics. By definition, the $(j, k)$ term of the kernel matrix generated by $K$ is 
		\begin{align}
			K_{j,k} &= K(x^{(j)}, x^{(k)}) \\
			&= K_1(x^{(j)}, x^{(k)}) + K_2(x^{(j)}, x^{(k)}) \\
			&= K_{1, j, k} + K_{2, j, k}
		\end{align}
		Therefore, $K = K_1 + K_2$. Because $K_1(\cdot)$ and $K_2(\cdot)$ are valid kernels, $K_1$ and $K_2$ are symmetric, so is $K$. Let $z \in \R^n$, note that both $K_1$ and $K_2$ are PSD, thus,
		\begin{align}
			z^T K z &= z^T K_1 z + z^T K_2 z \geq 0
		\end{align}
		Therefore, for any $(x\upi)_{i=1}^n \subset \R^d$, the corresponding kernel matrix generated by $K(\cdot)$ is symmetric and PSD, hence $K(\cdot)$ is a valid kernel.
	\end{proof}
	
	\newpage
	\subsection{3(b)}
	\begin{proof}
		$K(\cdot)$ is not necessarily a kernel. One counter-example can be constructed as following:
		\begin{align}
			K_1(x, y) &:= \id{x = y} \\
			K_2(x, y) &:= 2 \times \id{x = y}
		\end{align}
		Then for any $(x\upi)_{i=1}^n \subset \R^d$ sequence, the corresponding kernel matrices are
		\begin{align}
			K_1 &= I_n \\
			K_2 &= 2 I_n
		\end{align}
		Both $K_1$ and $K_2$ are symmetric and PSD, so $K_1(\cdot)$ and $K_2(\cdot)$ are valid kernels. Similarly to part (a), $K = K_1 - K_2$ for any $(x\upi)_{i=1}^n \subset \R^d$. In this case, for any $z \in \R^n$, $z^T K z \centernot{\geq} 0 $, so $K$ is not PSD. Hence, $K(\cdot)$ is not necessarily a valid kernel.
	\end{proof}
	
	\newpage
	\subsection{3(c)}
	\begin{proof}
		$K(\cdot)$ is a kernel. For any $(x\upi)_{i=1}^n \subset \R^d$, let $K_1$ denote the kernel matrix generated by kernel $K_1(\cdot)$, which is PSD and symmetric. And the kernel matrix $K = a K_1$ is symmetric. And because for every $z \in \R^n$, 
		\begin{align}
			z^T K_1 z \geq 0 &\implies z^T K_1 z \geq 0 \\
			&\implies z^T K z \geq 0
		\end{align}
		so $K$ is PSD and $K(\cdot)$ is a valid kernel.
	\end{proof}
	
	\newpage
	\subsection{3(d)}
	\begin{proof}
		$K(\cdot)$ is not a kernel. Let $K$ and $K_1$ denote the corresponding kernel matrices with some $(x\upi)_{i=1}^n \subset \R^d$. Then, $K_1$ is PSD. Therefore, for every $z \in \R^n$,
		\begin{align}
			z^T K_1 z \geq 0 &\implies (-a)z^T K_1 z \leq 0 \\
			&\implies z^T K z \leq 0
		\end{align}
		Therefore $K$ is not PSD, and $K(\cdot)$ is not a valid kernel.
	\end{proof}
	
	\newpage
	\subsection{3(e)}
	\begin{proof}
		$K(\cdot)$ is a kernel. Given that $K_1(\cdot)$ and $K_2(\cdot)$ are valid kernels, there exists some feature mappings corresponding to these kernels, say $\phi_1: \R^d \to \R^M$ and $\phi_2: \R^d \to \R^N$, where $M$ and $N$ are potentially infinite. Consider a collection $(x\upi)_{i=1}^n \subset \R^d$ and generated kernel matrices $K_1$, $K_2$, and $K$. Let $\phi_{1, \ell}(x\upi)$ denote the $\ell^{th}$ element of $\phi_1(x\upi)$. By definition,
		\begin{align}
			K_{i, j} &= K(x\upi, x\upj) \\
			&= K_1(x\upi, x^{(j)}) K_2(x\upi, x\upj) \\
			&= \inner{\phi_1(x\upi)}{\phi_1(x\upj)} \inner{\phi_2(x\upi)}{\phi_2(x\upj)} \\
			&= \left [
			\sum_{m=1}^M \phi_{1,m} (x\upi) \phi_{1,m}(x\upj)
			\right] \left[
			\sum_{n=1}^N \phi_{2,n} (x\upi) \phi_{2,n}(x\upj)
			\right] \\
			&= \sum_{m=1}^M 
			\left \{ \phi_{1,m} (x\upi) \phi_{1,m}(x\upj)
			\left[
			\sum_{n=1}^N \phi_{2,n} (x\upi) \phi_{2,n}(x\upj)
			\right] \right\} \\
			&= \sum_{m=1}^M \sum_{n=1}^N \left \{
			\phi_{1,m} (x\upi) \phi_{1,m}(x\upj)
			\phi_{2,n} (x\upi) \phi_{2,n}(x\upj)
			\right \} \\
			&= \sum_{m \in [M], n \in [N]}
			\phi_{1,m} (x\upi) \phi_{1,m}(x\upj)
			\phi_{2,n} (x\upi) \phi_{2,n}(x\upj)
		\end{align}
		Now we construct a new feature mapping $\phi_3: \R^d \to \R^{M \times N}$. For each component of $\phi(x\upi)$, it can be located using a double index $(m, n)$, where $m \in [M], n \in [N]$. Define $\phi_3$ as 
		\begin{align}
			\phi_{3, (m, n)}(x\upi) &:= \phi_{1, m} (x\upi) \phi_{2, n}(x\upi)
		\end{align}
		Let $\mc{I}$ denote the set of all $(m, n)$ indices for components of $\phi_3(x\upi)$, note that $|\mc{I}| = M \times N$, then 
		\begin{align}
			K_{i, j} &= \sum_{(m, n) \in \mc{I}}
			\phi_{1,m} (x\upi) \phi_{1,m}(x\upj)
			\phi_{2,n} (x\upi) \phi_{2,n}(x\upj) \\
			&= \sum_{(m, n) \in \mc{I}} \phi_{3, (m, n)}(x\upi) \phi_{3, (m, n)}(x\upj) \\
			&= \inner{\phi_3(x\upi)}{\phi_3(x\upj)}
		\end{align}
		And $K$ has been shown to be the the kernel matrix generated by feature mapping $\phi_3$, so $K(\cdot)$ is a valid kernel with corresponding feature mapping $\phi_3$.
	\end{proof}
	
	\newpage
	\subsection{3(f)}
	\begin{proof}
		$K(\cdot)$ is a kernel. Let $(x\upi)_{i=1}^n \subset \R^d$ be an arbitrary finite set, 
		\begin{align}
			K_{i, j} &= f(x\upi) f(x\upj) \\
			&= f(x\upj)f(x\upi) = K_{j, i}
		\end{align}
		So the generated kernel matrix is symmetric. Let $z \in \R^n$, then
		\begin{align}
			z^T K z &= \sum_{i=1}^n \sum_{j=1}^n z_i z_j K_{i, j} \\
			&= \sum_{i=1}^n \sum_{j=1}^n z_i f(x\upi) z_j f(x\upj) \\
			&= \inner{(f(x\upi))_{i=1}^n}{z}^2 \geq 0
		\end{align}
		Therefore, $K$ is PSD. So $K(\cdot)$ is a valid kernel.
	\end{proof}
	
	\newpage
	\subsection{3(g)}
	\begin{proof}
		$K(\cdot)$ is a valid kernel. Since $K_3$ is a kernel over $\R^p \times \R^p$, there exists a feature mapping $\lambda: \R^p \to \R^l$ for some $l$, such that $K_3(x, y) = \inner{\lambda(x)}{\lambda(y)}$ for every $x, y \in \R^p$. I am going to show that $K(\cdot)$ is a valid kernel on $\R^d \times \R^d$ by showing it can be written as an inner product of feature mapping $\lambda \circ \phi: \R^d \to \R^l$. Let $x, z \in \R^d$, then by definition
		\begin{align}
			K(x, z) &= K_3(\phi(x), \phi(z)) \\
			&= \inner{\lambda(\phi(x))}{\lambda(\phi(z))} \\
			&= \inner{\lambda \circ \phi(x)}{\lambda \circ \phi(z)}
		\end{align}
		Therefore $K(\cdot)$ is a valid kernel.
	\end{proof}
	
	\newpage
	\subsection{3(h)}
	\begin{proof}
		$K(\cdot)$ is a valid kernel. By the result from (e) and induction, it can be shown that $K(x, z) := \prod_i K_i(x, z)$, where all $K_i(\cdot)$ are kernels, is a valid kernel. This result can be reduced by setting all $K_i(\cdot)$ equal: let $K(\cdot)$ be a kernel, $n \in \N$, then $K'(x,z):=K(x,z)^n$ is also a kernel. Let $(\alpha_i)_{i=1}^P$ denote the coefficients of $p(\cdot)$, and $\alpha_i > 0$ for every $i$. Then by result from (c) and (e), for every $i$, $K_i(x, z) := \alpha_i K_1(x, z)^i$ is a valid kernel. And by result from (a) and induction, $K(x, z) := \sum_i K_i(x, z)$ is a valid kernel, therefore since
		\begin{align}
			K(x,z) &:= \sum_{i=1}^P \alpha_i K_1(x,z)^i = \sum_{i=1}^P K_i(x, z)
		\end{align}
		$K(\cdot)$ is also a valid kernel.
	\end{proof}
		
	\newpage
	\section{Problem 4 Kernelizing the Perceptron}
	\subsection{4(a)}
	\paragraph{Answer} The kernel trick firstly represents each $\theta\upi$ as a linear combination of features of observations:
	\begin{align}
		\theta\upi &= \sum_{\ell=1}^n \beta_\ell\upi \phi(x^{(\ell)})
	\end{align}
	$\theta^{(0)}$ can be represented simply by setting $\beta_\ell^{(0)} = 0$ for every $\ell$. To make prediction, $\theta\upi \cdot \phi(x^{(i+1)})$ is firstly computed as 
	\begin{align}
		\theta\upi \cdot \phi(x^{(i+1)}) &= \left[ \sum_{\ell=1}^n \beta_\ell\upi \phi(x^{(\ell)})\right] \cdot \phi(x^{(i+1)}) \\
		&= \sum_{\ell=1}^n \left[ \beta_\ell\upi \phi(x^{(\ell)}) \cdot \phi(x^{(i+1)}) \right] \\
		&= \sum_{\ell=1}^n \beta_\ell\upi K \left(\phi(x^{(\ell)}),\phi(x^{(i+1)}) \right) \\
	\end{align}
	The prediction is given by 
	\begin{align}
		h_{\theta\upi}(x^{(i+1)}) &= \tx{sign} \left (
			\sum_{\ell=1}^n \beta_\ell\upi K \left(\phi(x^{(\ell)}),\phi(x^{(i+1)}) \right)
		\right )
	\end{align}
	According to the given update rule, 
	\begin{align}
		\theta^{(i+1)} &= \sum_{\ell=1}^n \beta_\ell^{(i+1)} \phi(x^{(\ell)}) \\
		&:= \theta\upi + \alpha \left(y^{(i+1)} - h_{\theta\upi}(x^{(i+1)})\right) \phi(x^{(i+1)}) \\
		&= \sum_{\ell=1}^n \beta_\ell^{(i)} \phi(x^{(\ell)}) 
		+ \alpha \left(y^{(i+1)} - h_{\theta\upi}(x^{(i+1)})\right) \phi(x^{(i+1)})
	\end{align}
	Note that at this epoch, only $\beta_{i+1}$ is modified,
	\begin{align}
		\beta_{i+1}^{(i+1)} &:= \beta_{i+1}\upi + \alpha \left(y^{(i+1)} - h_{\theta\upi}(x^{(i+1)})\right) \\
		&= \beta_{i+1}\upi +\alpha \left(y^{(i+1)} - g(\theta\upi \cdot \phi(x^{(i+1)}) )\right)
	\end{align}
	For any $\ell \neq i + 1$,
	\begin{align}
		\beta_{\ell}^{(i+1)} := \beta_{\ell}^{(i)}
	\end{align}
	Given that $\beta^{(0)}_\ell$s are initialized to zero. And $\beta_{\ell}$ is updated at and only at step $\ell$. Therefore, $\beta_{i+1}\upi = \beta_{i+1}^{(0)} = 0$. The update rule can also be written as
	\begin{align}
		\beta_{i+1}^{(i+1)} &:= \alpha \left(y^{(i+1)} - g(\theta\upi \cdot \phi(x^{(i+1)}) )\right)
	\end{align}
	Moreover, because $\beta_\ell\upi = 0$ for every  $\ell > i$, the linear combination representation of $\theta\upi$ 
	\begin{align}
		\theta\upi &= \sum_{\ell=1}^n \beta_\ell\upi \phi(x^{(\ell)}) \\
		&= \sum_{\ell=1}^i \beta_\ell\upi \phi(x^{(\ell)}) + \sum_{\ell=i+1}^n \beta_\ell\upi \phi(x^{(\ell)}) \\
		&= \sum_{\ell=1}^i \beta_\ell\upi \phi(x^{(\ell)}) + 0 \\
		&= \sum_{\ell=1}^i \beta_\ell\upi \phi(x^{(\ell)})
	\end{align}
	and the prediction can be reduced to
	\begin{align}
		\theta\upi \cdot \phi(x^{(i+1)}) &= \tx{sign}\left(
		\sum_{\ell=1}^i \beta_\ell\upi K \left(\phi(x^{(\ell)}),\phi(x^{(i+1)}) \right)
		\right)
	\end{align}
	
	\newpage
	\subsection{4(b) Coding}
	\begin{figure}[h]
		\centering
		\includegraphics[width=0.6\linewidth]{src/perceptron/perceptron_dot_output.png}
		\caption{Classification with Dot Kernel}
	\end{figure}
	
	\begin{figure}[h]
		\centering
		\includegraphics[width=0.6\linewidth]{src/perceptron/perceptron_rbf_output.png}
		\caption{Classification with RBF Kernel}
	\end{figure}
	
	\newpage
	\subsection{4(c)}
	\paragraph{Answer} The dot kernel performed poorly. The dot kernel effectively exerted no feature engineering (i.e. $\phi(x) = x$). The prediction is then $\tx{sign}(\theta \cdot x)$. The classifier is seeking for a best separating hyperplane (i.e. a linear boundary) in the space of raw $x$ for classification. From the distribution of samples in the graph, this is obviously infeasible. And there might be a separating hyperplane in the feature space generated by feature mapping $\phi$ represented RBF kernel, so RBF kernel worked.
	
	\newpage
	\section{Problem 5 Neural Networks: MNIST image classification}
	\subsection{5(a)}
	\begin{figure}[h]
		\centering
		\includegraphics{src/mnist/baseline.pdf}
		\caption{Baseline Model Without Regularization}
	\end{figure}
	
	\newpage
	\subsection{5(b)}
	\begin{figure}[h]
		\centering
		\includegraphics{src/mnist/regularized.pdf}
		\caption{regularized Model}
	\end{figure}
	
	\newpage
	\subsection{5(c)}
	\paragraph{Output} 
	\begin{verbatim}
		For model baseline, got accuracy: 0.928700
		For model regularized, got accuracy: 0.967600
	\end{verbatim}
	\paragraph{Comment} From parts (a) and (b), the discrepancy between training and dev sets is greater with the baseline model, which suggests a high variance, and a potential overfitting problem. The accuracies from both model on the test test confirmed the overfitting problem: the baseline model performed well on training set, but poorly on dev and test set, so it overfitted the training data.
	
	\newpage
	\section{Problem 6 Bayesian Interpretation of Regularization}
	\subsection{6(a)}
	\begin{proof}
		Suppose $p(\theta) = p(\theta|x)$. Then, by Bayes' rule (all probabilities are conditioned on $x$),
		\begin{align}
			p(\theta|y, x) &= \frac{p(y|\theta, x) p(\theta|x)}{p(y|x)}
		\end{align}
		Therefore,
		\begin{align}
			\theta_{MAP} &:= \argmax_\theta p(\theta|y, x) \\
			&= \argmax_\theta \underbrace{\frac{1}{p(y|x)}}_{\tx{positive constant that } \perp \theta} p(y|\theta, x) p(\theta|x) \\
			&= \argmax_\theta p(y|\theta, x) p(\theta|x) \\
			&= \argmax_\theta p(y|\theta, x) p(\theta)
		\end{align}
	\end{proof}
	
	\newpage
	\subsection{6(b)}
	\begin{proof}
		Let $\theta \in \R^d$, and $\theta \sim \mc{N}(0, \eta^2 I_d)$.
		Note that $-\log(\cdot)$ is a negative and monotonic transformation, as a result, for every $f: \R \to \R$,
		\begin{align}
			\argmax_x f(x) &= \argmin_x - \log(f(x))
		\end{align}
		From (a),
		\begin{align}
			\theta_{MAP} &= \argmax_\theta p(y|\theta, x) p(\theta) \\
			&= \argmin_\theta - \log \left[p(y|\theta, x) p(\theta) \right] \\
			&= \argmin_\theta - \log(p(y|\theta, x)) - \log(p(\theta)) \\
			&= \argmin_\theta - \log(p(y|\theta, x)) - \log \left[
			\frac{1}{(2\pi)^{d/2} |\Sigma|^{1/2}} \exp \left (
			- \frac{1}{2} \theta^T \Sigma^{-1} \theta
			\right )
			\right ] \\
			&= \argmin_\theta - \log(p(y|\theta, x)) - \log (\Delta) + \frac{1}{2} \theta^T \frac{1}{\eta^2} I_d \theta
		\end{align}
		where $\Delta$ is constant independent from $\theta$. Hence,
		\begin{align}
			\dots &= \argmin_\theta - \log(p(y|\theta, x)) + \frac{1}{2 \eta^2} \theta^T \theta \\
			&= \argmin_\theta - \log(p(y|\theta, x)) + \lambda \norm{\theta}_2^2
		\end{align}
		where $\lambda := \frac{1}{2 \eta^2}$.
	\end{proof}
	
	\newpage
	\subsection{6(c)}
	\begin{proof}
		Since $(\varepsilon\upi) \overset{i.i.d}{\sim} \mc{N}(0, \sigma^2)$, let $\varepsilon \in \R^n$ denote the collection of all $\varepsilon\upi$, $\varepsilon \sim \mc{N}(0, \sigma^2 I_n)$. Let $X \in \R^{n \times d}$ and $y \in \R^n$, then 
		\begin{align}
			y|_{X, \theta} \sim \mc{N}(X \theta, \sigma^2 I_n)
		\end{align}
		From the previous sub-question,
		\begin{align}
			\theta_{MAP} &= \underbrace{\argmin_\theta - \log(p(y|\theta, X)) + \lambda \norm{\theta}_2^2}_{\mc{L}}
		\end{align}
		where $\mc{L}$ is concave because it is a summation of two concave functions, hence, the first order necessary condition for critical point is also sufficient for minimizer. Note that result from part (b) still holds when $y \in \R^n$ because its derivation does not depends on the dimensions of $y$ and $X$. $\mc{L}$ can be further reduced to
		\begin{align}
			\mc{L} &= - \log \left [
			\frac{1}{(2\pi)^{n/2} |\Sigma|^{1/2}} \exp \left (
			- \frac{1}{2} (y - X \theta)^T \Sigma^{-1} (y - X \theta)
			\right )
			\right] + \lambda \norm{\theta}_2^2 \\
			&= - \log \left (
			\frac{1}{(2\pi)^{n/2} |\sigma^2 I_n|^{1/2}}
			\right )
			+ \frac{1}{2} (y - X \theta)^T \frac{1}{\sigma^2} I_n (y - X \theta) 
			+ \lambda \norm{\theta}_2^2
		\end{align}
		Note that $- \log \left (
			\frac{1}{(2\pi)^{n/2} |\sigma^2 I_n|^{1/2}}
			\right )$ is constant against $\theta$, 
		\begin{align}
			\nabla_\theta \mc{L} = \frac{1}{2 \sigma^2} 2 (y - X\theta)^T (-X) + 2 \lambda \theta^T &= 0 \\
			\implies (y^T - \theta^T X^T)(-X) + 2 \sigma^2 \lambda \theta^T &= 0 \\
			\implies \theta^T X^TX + 2 \sigma^2 \lambda \theta^T&= y^T X \\
			\implies \theta^T (X^T X + 2 \sigma^2 \lambda I_n) &= y^T X
		\end{align}
		Note that $(X^T X + 2 \sigma^2 \lambda I_n)$ is symmetric and always invertible given $\sigma, \lambda \neq 0$. Take transpose on both sides
		\begin{align}
			(X^T X + 2 \sigma^2 \lambda I_n) \theta &= X^T y \\
			\implies \theta_{MAP} &= (X^T X + 2 \sigma^2 \lambda I_n)^{-1} X^T y
		\end{align}
	\end{proof}
	
	\newpage
	\subsection{6(d)}
	\begin{proof}
		Similarly to the argument used in part (c), 
		\begin{align}
			y|_{X, \theta} \sim \mc{N}(X \theta, \sigma^2 I_n)
		\end{align}
		Again, $-\log(\cdot)$ is a negative monotonic transformation, therefore, for every $f: \R \to \R$,
		\begin{align}
			\argmax_x f(x) &= \argmin_x - \log(f(x))
		\end{align}
		From part (a),
		\begin{align}
			\theta_{MAP} &= \argmax_\theta p(y|X, \theta) p(\theta) \\
			&= \argmin_\theta -\log \left[p(y|X, \theta) p(\theta)\right] \\
			&= \argmin_\theta -\log(p(y|X, \theta)) - \log(p(\theta)) \\
			&= \argmin_\theta -\log \left (
			\frac{1}{(2\pi)^{n/2} |\sigma^2 I_n|^{1/2}}
			\exp \left(
			-\frac{1}{2} (y - X\theta)^T \frac{1}{\sigma^2} I_n (y - X\theta)
			\right)
			\right) - \log(p(\theta)) \\
			&= \argmin_\theta -\log \left (\exp \left(
			-\frac{1}{2} (y - X\theta)^T \frac{1}{\sigma^2} I_n (y - X\theta)
			\right)
			\right) - \log(p(\theta)) \\
			&= \argmin_\theta \frac{1}{2} (y - X\theta)^T \frac{1}{\sigma^2} I_n (y - X\theta) - \log(p(\theta)) \\
			&= \argmin_\theta \frac{1}{2 \sigma^2} \norm{y - X\theta}^2_2 - \log(p(\theta))
		\end{align}
		Since $\theta_i \overset{i.i.d}{\sim} \mc{L}(0, b)$, 
		\begin{align}
			\log(p(\theta)) &= \log \left( \prod_{i=1}^d p(\theta_i) \right) \\
			&= \sum_{i=1}^d \log(f_{\mc{L}} (\theta_i|0, b)) \\
			&= \sum_{i=1}^d \log\left(\frac{1}{2b} \exp \left(- \frac{|\theta_i|}{b} \right) \right) \\
			&= \sum_{i=1}^d \left \{
			\log(\frac{1}{2b}) - \frac{|\theta_i|}{b}
			\right \} \\
			&= -d\log(2b) - \frac{1}{b} \sum_{i=1}^d |\theta_i| \\
			&= -d\log(2b) - \frac{1}{b} \norm{\theta}_1
		\end{align}
		Therefore,
		\begin{align}
			\theta_{MAP} &= \argmin_\theta \frac{1}{2\sigma^2} \norm{y - X\theta}_2^2 + d \log(2b) + \frac{1}{b} \norm{\theta}_1 \\
			&= \argmin_\theta \norm{y - X\theta}_2^2 + \frac{2\sigma^2}{b} \norm{\theta}_1 \\
			&= \argmin_\theta \norm{y - X\theta}_2^2 + \gamma \norm{\theta}_1 \\
			&= \argmin_\theta J(\theta)
		\end{align}
		where $\gamma := \frac{2\sigma^2}{b}$.
	\end{proof}
\end{document}



















